The partner needed to replicate the blind source separation algorithm used in @negro_muceli_castronovo_holobar_farina_2016 to decompose raw EMG signals into their constituent MU spike trains (MUSTs). This algorithm provides advantages to other blind source separation algorithms in that it is an experimentally validated approach that allows for the decomposition of multi-channel invasive EMG and non-invasive EMG data.

(ref:separation-vectors) Separation vector extraction process performed by the blind source separation algorithm. w(1) is the separation vector at the first iteration of the latent component analysis step, where the red line is the current estimation and the gray line is the theoretical ideal separation vector. s(1) is the separation vector's corresponding MUST. w(n) is the estimated separation vector at the final iteration of the latent component analysis step, and s(n) is its corresponding MUST. Figure modified from @negro_muceli_castronovo_holobar_farina_2016.

```{r separation-vectors, echo=FALSE, fig.cap="(ref:separation-vectors)", fig.align="center", out.width="80%"}
knitr::include_graphics(path="images/separation-vectors.png")
```

Essentially, the blind source separation algorithm is an unsupervised machine learning model that iteratively extracts separation vectors from the data. Separation vectors are vectors that, when applied to the pre-processed data, separate a single MUST from unwanted noise and other MUSTs, as seen in **figure \@ref(fig:separation-vectors)**. An MUST contains the firing times of a single MU, and it is the mixture of these that compose the raw EMG signal.

Separation vectors are extracted through broadly three steps:

1.  The data is pre-processed.
2.  Latent component analysis (LCA) is performed to estimate a separation vector.
3.  The refinement process is used to increase the quality of the estimated separation vector.

Steps two and three are repeated for a predetermined amount of iterations of the overall blind source separation algorithm.

(ref:data-preprocessing) The data pre-processing pipeline. a. The original raw data, x, is band-pass filtered. b. The data of each channel is centred around 0. c. The data is extended to form x`, with covariance matrix, cov(x', x'). d. The data is whitened to form z, with covariance matrix, cov(z, z). Figure modified from @negro_muceli_castronovo_holobar_farina_2016.

```{r data-preprocessing, echo=FALSE, fig.cap="(ref:data-preprocessing)", fig.align="center", out.width="90%"}
knitr::include_graphics(path="images/data-preprocessing.png")
```

**Step 1: Pre-Processing.**

The first step of the algorithm is data pre-processing, which consists of four sub-steps (**figure \@ref(fig:data-preprocessing)**):

**a.** First, the data is *band-pass filtered.* This removes noise from the data and increases the number of identified separation vectors.

**b.** Then, the data is *centred* by subtracting the mean of each channel per channel. This process is essential for LCA to extract all of the separation vectors.

**c.** Then each channel is *extended* by a certain number of time-delayed versions of that channel. This process converts the mixture of signals from a convolutive mixture to a linear instantaneous mixture, which is mathematically necessary for LCA to work [@negro_muceli_castronovo_holobar_farina_2016; @weenink_2012].

**d.** Finally, the data is *whitened*, so that the covariance matrix of the extended channels is equal to the identity matrix. This improves performance by computationally simplifying the convergence of the LCA step [@hyv√§rinen_karhunen_oja_2001].

**Step 2: Latent Component Analysis.**

The pre-processed data then goes through the LCA step. The LCA step is based on independent component analysis, where the separation vectors are obtained by maximizing their statistical independence from each other. However, since the MUSTs are extended along with the observations, they cannot be fully independent. This is why LCA extracts separation vectors by maximizing sparsity instead of independence. Intuitively, this is done because a singular MUST will be more sparse than the combination of multiple MUSTs [@negro_muceli_castronovo_holobar_farina_2016]. As seen in **figure \@ref(fig:separation-vectors)**, the MUST is much more sparse after the final iteration of LCA in comparison to the first iteration.

**a.** First, the separation vector is *initialized* by using a time instance of high activity in the whitened data, as these time instances are likely to correspond to multiple MU firings.

**b.** Using a contrast function that measures the sparsity of the MUST, the estimated separation vector is *iteratively updated*. In each iteration, the estimated separation vectors are orthogonalized against previously accepted separation vectors and normalized, to increase the number of extracted unique MUSTs.

**c.** The LCA step converges. This happens when the separation vector approximately no longer changes between iterations.

**Step 3: Refinement.**

After the separation vector is extracted, it goes through the refinement step. This is done because LCA may converge to unreliable estimates and through refinement, the quality of the estimate is increased. The refinement step is an iterative algorithm that maximizes the regularity of the MUST under the assumption that singular MUs fire off action potentials at a much more regular rate than combinations of MUs [@negro_muceli_castronovo_holobar_farina_2016].

**a.** First, the MUST is estimated by applying the separation vector to the pre-processed data.

**b.** Then, the firing times are determined by applying the peak-finding algorithm from @2020SciPy-NMeth. Of these firing times, the instances that correspond to small peaks in the spike train are separated away from the large peaks using the KMeans algorithm from @scikit-learn. The firing times corresponding to small peaks are discarded as they likely correspond to the firing occurrence of more than one MU [@negro_muceli_castronovo_holobar_farina_2016]. The information from the accepted firing times is used to update the separation vector.

**c.** The iterative refinement process converges once the coefficient of variation of the time between firings increases.

\begin{equation} 
  \text{silhouette score} = \frac{b - a}{max(a, b)}
  (\#eq:sil)
\end{equation}

$$a = \text{inter-cluster sum of point-to-centroid distances}$$
$$b = \text{intra-cluster sum of point-to-centroid distances}$$

\begin{equation} 
  \text{pulse-to-noise ratio} = 10\log_{10} \bigg(\frac{P_{\text{Signal}}}{P_{\text{Noise}}} \bigg)
  (\#eq:pnr)
\end{equation}

$$P_{\text{Signal}} = \text{Power of signal}$$
$$P_{\text{Noise}} = \text{Power of noise}$$

Once the refinement process is done, the refined separation vector is accepted based on a user-defined threshold of either the silhouette score between the signal and the noise or the pulse-to-noise ratio. The silhouette score is defined in **\@ref(eq:sil)** and is calculated using the signal and noise clusters in the MUSTs [@negro_muceli_castronovo_holobar_farina_2016]. The pulse-to-noise ratio is defined in **\@ref(eq:pnr)**. The accepted separation vectors correspond to the MUs that the blind source separation algorithm extracts from the raw signal.

A further improvement to the algorithm that we did not have time to implement would be a re-learning feature. The user would run the algorithm on a sample of the data, and then identify inaccurate firing times (false positives) based on physiological limits of MU firing rates. The algorithm would use this information to no longer make similar mistakes in the rest of the decomposition. Implementing this feature would be quite complex because it is unclear how this would be implemented programmatically.

The stakeholders affected by our blind source separation algorithm are researchers and those that would be affected by their research. This is why our algorithm must work properly so that researchers' results are accurate and do not affect the general public adversely down the line. For example, if someone uses `EMGdecomPy` and obtains inaccurate results which are  used to inform a neuromuscular diagnosis, it could greatly affect someone's life. Periodically, the results of our algorithm should be compared to others to obtain a second opinion on the decomposition of the EMG signal. 

The data provided by @Hug2021 contains EMGs and their decomposition results from many different muscles at different voluntary muscle contraction intensities (100% being the most intensely the subject can contract their muscle). We have not had the chance to thoroughly validate our algorithm using this data, as the debugging process took a great deal of time. We have only received qualitative results, obtained by visually comparing MUAP shapes identified by `EMGdecomPy` and those @Hug2021 identified using `DEMUSE`, a commercial software created by @holobar_2016. There are concerns with this approach as `DEMUSE` uses a similar but different algorithm than @negro_muceli_castronovo_holobar_farina_2016. `DEMUSE` is a highly used software in comparison to `OTBioLab+`, and therefore the partner wishes to compare our results to theirs.

\begin{equation} 
  \text{rate of agreement} = \frac{c_j}{c_j + A_j +B_j}
  (\#eq:roa)
\end{equation}


$$c_j = \text{Number of discharges (MU firings) for the j-th MU, identified by both algorithms}$$
$$A_j = \text{Number of discharges for the j-th MU, identified by one of the algorithms}$$
$$B_j = \text{Number of discharges for the j-th MU, identified by the other algorithm}$$

In the future, we would like to quantitatively validate our results on more muscle groups, using the metric recommended by @negro_muceli_castronovo_holobar_farina_2016, the rate of agreement (RoA), defined in **\@ref(eq:roa)**. RoA theoretically ranges from 0 to 1. The closer the RoA is to 1 the more the decomposition provided by `EMGdecomPy` matches the one provided by @Hug2021 for the j-th MU. Therefore getting a high RoA would mean that `EMGdecomPy` is approximately as accurate at decomposing raw EMG signals as the `DEMUSE` software for the @Hug2021 datasets.
